{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 8: Implement Your Machine Learning Project Plan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab assignment, you will implement the machine learning project plan you created in the written assignment. You will:\n",
    "\n",
    "1. Load your data set and save it to a Pandas DataFrame.\n",
    "2. Perform exploratory data analysis on your data to determine which feature engineering and data preparation techniques you will use.\n",
    "3. Prepare your data for your model and create features and a label.\n",
    "4. Fit your model to the training data and evaluate your model.\n",
    "5. Improve your model by performing model selection and/or feature selection techniques to find best model for your problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages\n",
    "\n",
    "Before you get started, import a few packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Task:</b> In the code cell below, import additional packages that you have used in this course that you will need for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Load the Data Set\n",
    "\n",
    "\n",
    "You have chosen to work with one of four data sets. The data sets are located in a folder named \"data.\" The file names of the three data sets are as follows:\n",
    "\n",
    "* The \"adult\" data set that contains Census information from 1994 is located in file `adultData.csv`\n",
    "* The airbnb NYC \"listings\" data set is located in file  `airbnbListingsData.csv`\n",
    "* The World Happiness Report (WHR) data set is located in file `WHR2018Chapter2OnlineData.csv`\n",
    "* The book review data set is located in file `bookReviewsData.csv`\n",
    "\n",
    "\n",
    "\n",
    "<b>Task:</b> In the code cell below, use the same method you have been using to load your data using `pd.read_csv()` and save it to DataFrame `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Positive Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This was perhaps the best of Johannes Steinhof...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This very fascinating book is a story written ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The four tales in this collection are beautifu...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The book contained more profanity than I expec...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We have now entered a second time of deep conc...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Positive Review\n",
       "0  This was perhaps the best of Johannes Steinhof...             True\n",
       "1  This very fascinating book is a story written ...             True\n",
       "2  The four tales in this collection are beautifu...             True\n",
       "3  The book contained more profanity than I expec...            False\n",
       "4  We have now entered a second time of deep conc...             True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "filename = os.path.join(os.getcwd(), \"data\", \"bookReviewsData.csv\")\n",
    "\n",
    "df = pd.read_csv(filename, header=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Exploratory Data Analysis\n",
    "\n",
    "The next step is to inspect and analyze your data set with your machine learning problem and project plan in mind. \n",
    "\n",
    "This step will help you determine data preparation and feature engineering techniques you will need to apply to your data to build a balanced modeling data set for your problem and model. These data preparation techniques may include:\n",
    "* addressing missingness, such as replacing missing values with means\n",
    "* renaming features and labels\n",
    "* finding and replacing outliers\n",
    "* performing winsorization if needed\n",
    "* performing one-hot encoding on categorical features\n",
    "* performing vectorization for an NLP problem\n",
    "* addressing class imbalance in your data sample to promote fair AI\n",
    "\n",
    "\n",
    "Think of the different techniques you have used to inspect and analyze your data in this course. These include using Pandas to apply data filters, using the Pandas `describe()` method to get insight into key statistics for each column, using the Pandas `dtypes` property to inspect the data type of each column, and using Matplotlib and Seaborn to detect outliers and visualize relationships between features and labels. If you are working on a classification problem, use techniques you have learned to determine if there is class imbalance.\n",
    "\n",
    "\n",
    "<b>Task</b>: Use the techniques you have learned in this course to inspect and analyze your data. \n",
    "\n",
    "<b>Note</b>: You can add code cells if needed by going to the <b>Insert</b> menu and clicking on <b>Insert Cell Below</b> in the drop-drown menu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Positive Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1973</td>\n",
       "      <td>1973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>1865</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>How can a best-selling author like Simon Winch...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>3</td>\n",
       "      <td>993</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Review Positive Review\n",
       "count                                                1973            1973\n",
       "unique                                               1865               2\n",
       "top     How can a best-selling author like Simon Winch...           False\n",
       "freq                                                    3             993"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Review             object\n",
       "Positive Review      bool\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1973 entries, 0 to 1972\n",
      "Data columns (total 2 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   Review           1973 non-null   object\n",
      " 1   Positive Review  1973 non-null   bool  \n",
      "dtypes: bool(1), object(1)\n",
      "memory usage: 17.5+ KB\n"
     ]
    }
   ],
   "source": [
    "# this tells us that there are no missing values\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    993\n",
       "True     980\n",
       "Name: Positive Review, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for class imbalance\n",
    "df['Positive Review'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'Class Distribution'}>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check for class imbalance visually\n",
    "\n",
    "# count occurence of each class\n",
    "class_counts = df['Positive Review'].value_counts()\n",
    "\n",
    "# visualize class distribution\n",
    "class_counts.plot(kind='bar', title='Class Distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average document length: 933.3360364926508\n",
      "Minimum document length: 59\n",
      "Maximum document length: 32467\n"
     ]
    }
   ],
   "source": [
    "# explore feature space of text and corpus space\n",
    "\n",
    "text = df['Review']\n",
    "doc_lengths = [len(doc) for doc in text]\n",
    "\n",
    "print(\"Average document length:\", sum(doc_lengths) / len(doc_lengths))\n",
    "print(\"Minimum document length:\", min(doc_lengths))\n",
    "print(\"Maximum document length:\", max(doc_lengths))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Implement Your Project Plan\n",
    "\n",
    "<b>Task:</b> Use the rest of this notebook to carry out your project plan. You will:\n",
    "\n",
    "1. Prepare your data for your model and create features and a label.\n",
    "2. Fit your model to the training data and evaluate your model.\n",
    "3. Improve your model by performing model selection and/or feature selection techniques to find best model for your problem.\n",
    "\n",
    "\n",
    "Add code cells below and populate the notebook with commentary, code, analyses, results, and figures as you see fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create features and a label\n",
    "\n",
    "# label \n",
    "y = df['Positive Review'] \n",
    "\n",
    "# features\n",
    "X = df['Review']\n",
    "\n",
    "# split into training, validation, and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=.75, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preparation \n",
    "# tokenize data\n",
    "\n",
    "# create TfidfVectorizer oject\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# fit vectorizer to X_train\n",
    "tfidf_vectorizer.fit(X_train)\n",
    "\n",
    "# transform training and test data using the fitted vectorizer\n",
    "X_train_tfidf = tfidf_vectorizer.transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on the test data: 0.9146\n",
      "The size of the feature space: 18558\n",
      "Glimpse of first 5 entries of the mapping of a word to its column/feature index \n",
      "[('is', 9043), ('reason', 13533), ('this', 16714), ('book', 2189)]:\n"
     ]
    }
   ],
   "source": [
    "# fit model to training data\n",
    "\n",
    "# create LogisticRegression model object, fit to the transformed training data\n",
    "model = LogisticRegression(max_iter=200)\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# make predictions on the transformed test data\n",
    "probability_predictions = model.predict_proba(X_test_tfidf)[:,1]\n",
    "\n",
    "# make predictions on the transformed test data \n",
    "class_label_predictions = model.predict(X_test_tfidf)\n",
    "\n",
    "# compute AUC for test data\n",
    "auc = roc_auc_score(y_test, probability_predictions)\n",
    "print('AUC on the test data: {:.4f}'.format(auc))\n",
    "\n",
    "# print size of resulting feature space\n",
    "len_feature_space = len(tfidf_vectorizer.vocabulary_)\n",
    "print('The size of the feature space: {0}'.format(len_feature_space))\n",
    "\n",
    "# get glimpse of features\n",
    "first_five = list(tfidf_vectorizer.vocabulary_.items())[1:5]\n",
    "print('Glimpse of first 5 entries of the mapping of a word to its column/feature index \\n{}:'.format(first_five))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Min Document Frequency Value: 1\n",
      "AUC on the test data: 0.9268\n",
      "The size of the feature space: 138486\n",
      "Glimpse of first 5 entries of the mapping of a word to its column/feature index \n",
      "[('is', 61671), ('reason', 97323), ('this', 120815), ('book', 18054)]:\n",
      "Glimpse of first 5 stop words \n",
      "[]:\n",
      "\n",
      "Min Document Frequency Value: 3\n",
      "AUC on the test data: 0.9280\n",
      "The size of the feature space: 17684\n",
      "Glimpse of first 5 entries of the mapping of a word to its column/feature index \n",
      "[('is', 7641), ('reason', 11976), ('this', 15162), ('book', 2272)]:\n",
      "Glimpse of first 5 stop words \n",
      "['noticeably', 'apartment you', 'future works', 'software black']:\n",
      "\n",
      "Min Document Frequency Value: 5\n",
      "AUC on the test data: 0.9277\n",
      "The size of the feature space: 9140\n",
      "Glimpse of first 5 entries of the mapping of a word to its column/feature index \n",
      "[('is', 3944), ('reason', 6148), ('this', 7795), ('book', 1116)]:\n",
      "Glimpse of first 5 stop words \n",
      "['noticeably', 'apartment you', 'future works', 'software black']:\n",
      "\n",
      "Min Document Frequency Value: 10\n",
      "AUC on the test data: 0.9195\n",
      "The size of the feature space: 4023\n",
      "Glimpse of first 5 entries of the mapping of a word to its column/feature index \n",
      "[('is', 1687), ('reason', 2699), ('this', 3396), ('book', 464)]:\n",
      "Glimpse of first 5 stop words \n",
      "['noticeably', 'page the', 'apartment you', 'future works']:\n",
      "\n",
      "Min Document Frequency Value: 100\n",
      "AUC on the test data: 0.8463\n",
      "The size of the feature space: 257\n",
      "Glimpse of first 5 entries of the mapping of a word to its column/feature index \n",
      "[('is', 102), ('this', 207), ('book', 35), ('has', 81)]:\n",
      "Glimpse of first 5 stop words \n",
      "['noticeably', 'page the', 'apartment you', 'future works']:\n",
      "\n",
      "Min Document Frequency Value: 500\n",
      "AUC on the test data: 0.7323\n",
      "The size of the feature space: 32\n",
      "Glimpse of first 5 entries of the mapping of a word to its column/feature index \n",
      "[('this', 26), ('book', 7), ('it', 16), ('to', 28)]:\n",
      "Glimpse of first 5 stop words \n",
      "['noticeably', 'page the', 'apartment you', 'future works']:\n",
      "\n",
      "Min Document Frequency Value: 1000\n",
      "AUC on the test data: 0.6435\n",
      "The size of the feature space: 9\n",
      "Glimpse of first 5 entries of the mapping of a word to its column/feature index \n",
      "[('this', 7), ('book', 1), ('it', 4), ('to', 8)]:\n",
      "Glimpse of first 5 stop words \n",
      "['noticeably', 'page the', 'apartment you', 'future works']:\n"
     ]
    }
   ],
   "source": [
    "# evaluate logistic regression model by varying min_dif values\n",
    "\n",
    "for min_df in [1,3,5,10,100,500,1000]:\n",
    "    \n",
    "    print('\\nMin Document Frequency Value: {0}'.format(min_df))\n",
    "    \n",
    "    # create TfidfVectorizer oject\n",
    "    tfidf_vectorizer = TfidfVectorizer(min_df=min_df, ngram_range=(1,2))\n",
    "\n",
    "    # fit vectorizer to X_train\n",
    "    tfidf_vectorizer.fit(X_train)\n",
    "\n",
    "    # transform the training and test data\n",
    "    X_train_tfidf = tfidf_vectorizer.transform(X_train)\n",
    "    X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "    # create a LogisticRegression model object, and fit to the transformed training data\n",
    "    model = LogisticRegression(max_iter=200)\n",
    "    model.fit(X_train_tfidf, y_train)\n",
    "    \n",
    "    # make predictions on the transformed test data\n",
    "    probability_predictions = model.predict_proba(X_test_tfidf)[:,1]\n",
    "\n",
    "    # compute AUC for the test data\n",
    "    auc = roc_auc_score(y_test, probability_predictions)\n",
    "    print('AUC on the test data: {:.4f}'.format(auc))\n",
    "\n",
    "    # compute the size of the resulting feature space\n",
    "    len_feature_space = len(tfidf_vectorizer.vocabulary_)\n",
    "    print('The size of the feature space: {0}'.format(len_feature_space))\n",
    "    \n",
    "    # get a glimpse of the features\n",
    "    first_five = list(tfidf_vectorizer.vocabulary_.items())[1:5]\n",
    "    print('Glimpse of first 5 entries of the mapping of a word to its column/feature index \\n{}:'.format(first_five))\n",
    "\n",
    "    # print the first five stop words\n",
    "    first_five_stop = list(tfidf_vectorizer.stop_words_)[1:5]\n",
    "    print('Glimpse of first 5 stop words \\n{}:'.format(first_five_stop))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Min Document Frequency Value: 1\n",
      "\n",
      "Max Document Frequency Value: 0.5\n",
      "AUC on the test data: 0.9263\n",
      "The size of the feature space: 138471\n",
      "Glimpse of first 5 entries of the mapping of a word to its column/feature index \n",
      "[('reason', 97314), ('has', 51655), ('sold', 107012), ('over', 87204)]:\n",
      "Glimpse of first 5 stop words \n",
      "['book', 'with', 'and', 'to']:\n",
      "\n",
      "Min Document Frequency Value: 1\n",
      "\n",
      "Max Document Frequency Value: 0.7\n",
      "AUC on the test data: 0.9269\n",
      "The size of the feature space: 138477\n",
      "Glimpse of first 5 entries of the mapping of a word to its column/feature index \n",
      "[('reason', 97317), ('has', 51657), ('sold', 107015), ('over', 87207)]:\n",
      "Glimpse of first 5 stop words \n",
      "['book', 'and', 'to', 'in']:\n",
      "\n",
      "Min Document Frequency Value: 1\n",
      "\n",
      "Max Document Frequency Value: 0.9\n",
      "AUC on the test data: 0.9269\n",
      "The size of the feature space: 138485\n",
      "Glimpse of first 5 entries of the mapping of a word to its column/feature index \n",
      "[('is', 61671), ('reason', 97323), ('this', 120814), ('book', 18054)]:\n",
      "Glimpse of first 5 stop words \n",
      "[]:\n",
      "\n",
      "Min Document Frequency Value: 1\n",
      "\n",
      "Max Document Frequency Value: 1.0\n",
      "AUC on the test data: 0.9268\n",
      "The size of the feature space: 138486\n",
      "Glimpse of first 5 entries of the mapping of a word to its column/feature index \n",
      "[('is', 61671), ('reason', 97323), ('this', 120815), ('book', 18054)]:\n",
      "Glimpse of first 5 stop words \n",
      "[]:\n",
      "\n",
      "Min Document Frequency Value: 3\n",
      "\n",
      "Max Document Frequency Value: 0.5\n",
      "AUC on the test data: 0.9292\n",
      "The size of the feature space: 17669\n",
      "Glimpse of first 5 entries of the mapping of a word to its column/feature index \n",
      "[('reason', 11967), ('has', 6291), ('sold', 13059), ('over', 10836)]:\n",
      "Glimpse of first 5 stop words \n",
      "['noticeably', 'apartment you', 'future works', 'software black']:\n",
      "\n",
      "Min Document Frequency Value: 3\n",
      "\n",
      "Max Document Frequency Value: 0.7\n",
      "AUC on the test data: 0.9301\n",
      "The size of the feature space: 17675\n",
      "Glimpse of first 5 entries of the mapping of a word to its column/feature index \n",
      "[('reason', 11970), ('has', 6293), ('sold', 13062), ('over', 10839)]:\n",
      "Glimpse of first 5 stop words \n",
      "['noticeably', 'apartment you', 'future works', 'software black']:\n",
      "\n",
      "Min Document Frequency Value: 3\n",
      "\n",
      "Max Document Frequency Value: 0.9\n",
      "AUC on the test data: 0.9281\n",
      "The size of the feature space: 17683\n",
      "Glimpse of first 5 entries of the mapping of a word to its column/feature index \n",
      "[('is', 7641), ('reason', 11976), ('this', 15161), ('book', 2272)]:\n",
      "Glimpse of first 5 stop words \n",
      "['noticeably', 'apartment you', 'future works', 'software black']:\n",
      "\n",
      "Min Document Frequency Value: 3\n",
      "\n",
      "Max Document Frequency Value: 1.0\n",
      "AUC on the test data: 0.9280\n",
      "The size of the feature space: 17684\n",
      "Glimpse of first 5 entries of the mapping of a word to its column/feature index \n",
      "[('is', 7641), ('reason', 11976), ('this', 15162), ('book', 2272)]:\n",
      "Glimpse of first 5 stop words \n",
      "['noticeably', 'apartment you', 'future works', 'software black']:\n",
      "\n",
      "Min Document Frequency Value: 100\n",
      "\n",
      "Max Document Frequency Value: 0.5\n",
      "AUC on the test data: 0.8319\n",
      "The size of the feature space: 242\n",
      "Glimpse of first 5 entries of the mapping of a word to its column/feature index \n",
      "[('has', 77), ('over', 145), ('right', 159), ('point', 149)]:\n",
      "Glimpse of first 5 stop words \n",
      "['noticeably', 'page the', 'apartment you', 'future works']:\n",
      "\n",
      "Min Document Frequency Value: 100\n",
      "\n",
      "Max Document Frequency Value: 0.7\n",
      "AUC on the test data: 0.8383\n",
      "The size of the feature space: 248\n",
      "Glimpse of first 5 entries of the mapping of a word to its column/feature index \n",
      "[('has', 79), ('over', 148), ('right', 162), ('point', 152)]:\n",
      "Glimpse of first 5 stop words \n",
      "['noticeably', 'page the', 'apartment you', 'future works']:\n",
      "\n",
      "Min Document Frequency Value: 100\n",
      "\n",
      "Max Document Frequency Value: 0.9\n",
      "AUC on the test data: 0.8468\n",
      "The size of the feature space: 256\n",
      "Glimpse of first 5 entries of the mapping of a word to its column/feature index \n",
      "[('is', 102), ('this', 206), ('book', 35), ('has', 81)]:\n",
      "Glimpse of first 5 stop words \n",
      "['noticeably', 'page the', 'apartment you', 'future works']:\n",
      "\n",
      "Min Document Frequency Value: 100\n",
      "\n",
      "Max Document Frequency Value: 1.0\n",
      "AUC on the test data: 0.8463\n",
      "The size of the feature space: 257\n",
      "Glimpse of first 5 entries of the mapping of a word to its column/feature index \n",
      "[('is', 102), ('this', 207), ('book', 35), ('has', 81)]:\n",
      "Glimpse of first 5 stop words \n",
      "['noticeably', 'page the', 'apartment you', 'future works']:\n",
      "\n",
      "Min Document Frequency Value: 1000\n",
      "\n",
      "Max Document Frequency Value: 0.5\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "max_df corresponds to < documents than min_df",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-adf732be001c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# fit vectorizer to X_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mtfidf_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# transform training and test data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1834\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1835\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_warn_for_unused_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1836\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1837\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1838\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1235\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmax_doc_count\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmin_doc_count\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1236\u001b[0m                 raise ValueError(\n\u001b[0;32m-> 1237\u001b[0;31m                     \"max_df corresponds to < documents than min_df\")\n\u001b[0m\u001b[1;32m   1238\u001b[0m             X, self.stop_words_ = self._limit_features(X, vocabulary,\n\u001b[1;32m   1239\u001b[0m                                                        \u001b[0mmax_doc_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: max_df corresponds to < documents than min_df"
     ]
    }
   ],
   "source": [
    "# evaluate logistic regression model by varying BOTH min_df and max_df values\n",
    "\n",
    "min_df_values = [1,3,100,1000]\n",
    "max_df_values = [0.5, 0.7, 0.9, 1.0]\n",
    "\n",
    "for min_df in min_df_values:\n",
    "    for max_df in max_df_values:\n",
    "    \n",
    "        print('\\nMin Document Frequency Value: {0}'.format(min_df))\n",
    "        print('\\nMax Document Frequency Value: {0}'.format(max_df))\n",
    "\n",
    "        # create TfidfVectorizer oject\n",
    "        tfidf_vectorizer = TfidfVectorizer(min_df=min_df, max_df=max_df, ngram_range=(1,2))\n",
    "\n",
    "        # fit vectorizer to X_train\n",
    "        tfidf_vectorizer.fit(X_train)\n",
    "\n",
    "        # transform training and test data\n",
    "        X_train_tfidf = tfidf_vectorizer.transform(X_train)\n",
    "        X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "        # create a LogisticRegression model object, fit model to the transformed training data\n",
    "        model = LogisticRegression(max_iter=200)\n",
    "        model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "        # make predictions on the transformed test data\n",
    "        probability_predictions = model.predict_proba(X_test_tfidf)[:,1]\n",
    "\n",
    "        # compute the AUC for test data\n",
    "        auc = roc_auc_score(y_test, probability_predictions)\n",
    "        print('AUC on the test data: {:.4f}'.format(auc))\n",
    "\n",
    "        # compute the size of the resulting feature space\n",
    "        len_feature_space = len(tfidf_vectorizer.vocabulary_)\n",
    "        print('The size of the feature space: {0}'.format(len_feature_space))\n",
    "\n",
    "        # get a glimpse of the features\n",
    "        first_five = list(tfidf_vectorizer.vocabulary_.items())[1:5]\n",
    "        print('Glimpse of first 5 entries of the mapping of a word to its column/feature index \\n{}:'.format(first_five))\n",
    "\n",
    "        # print the first five stop words\n",
    "        first_five_stop = list(tfidf_vectorizer.stop_words_)[1:5]\n",
    "        print('Glimpse of first 5 stop words \\n{}:'.format(first_five_stop))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis \n",
    "\n",
    "After varying both the min_df value and max_df value and testing the different combinations, I found that the value of 3 for min_df and 0.7 for max_df provided the best AUC: 0.9301. The AUC measures the ability of a binary classifier to distinguish between classes. The higher the AUC, the better the model's performance at distinguishing between the positive and negative classes. In this case, this specific combination of min_df and max_df resulted in an AUC of 0.9301 which indicates that the model is able to distinguish between the positive and negative book reviews. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Min Document Frequency Value: 1000\n",
      "AUC on the test data: 0.9301\n",
      "The size of the feature space: 17675\n",
      "Glimpse of first 5 entries of the mapping of a word to its column/feature index \n",
      "[('reason', 11970), ('has', 6293), ('sold', 13062), ('over', 10839)]:\n",
      "Glimpse of first 5 stop words \n",
      "['noticeably', 'apartment you', 'future works', 'software black']:\n"
     ]
    }
   ],
   "source": [
    "# improve the model with best min_df and max_df values (same steps as previous)\n",
    "\n",
    "print('\\nMin Document Frequency Value: {0}'.format(min_df))\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df=3, max_df=0.7, ngram_range=(1,2))\n",
    "\n",
    "tfidf_vectorizer.fit(X_train)\n",
    "\n",
    "X_train_tfidf = tfidf_vectorizer.transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "    \n",
    "model = LogisticRegression(max_iter=200)\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "    \n",
    "probability_predictions = model.predict_proba(X_test_tfidf)[:,1]\n",
    "\n",
    "auc = roc_auc_score(y_test, probability_predictions)\n",
    "print('AUC on the test data: {:.4f}'.format(auc))\n",
    "   \n",
    "len_feature_space = len(tfidf_vectorizer.vocabulary_)\n",
    "print('The size of the feature space: {0}'.format(len_feature_space))\n",
    "    \n",
    "first_five = list(tfidf_vectorizer.vocabulary_.items())[1:5]\n",
    "print('Glimpse of first 5 entries of the mapping of a word to its column/feature index \\n{}:'.format(first_five))\n",
    "\n",
    "first_five_stop = list(tfidf_vectorizer.stop_words_)[1:5]\n",
    "print('Glimpse of first 5 stop words \\n{}:'.format(first_five_stop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for the first 15 examples:\n",
      "Probability\t\t\tClass\n",
      "True\t\t\tGood Review\n",
      "True\t\t\tGood Review\n",
      "False\t\t\tBad Review\n",
      "True\t\t\tGood Review\n",
      "True\t\t\tGood Review\n",
      "False\t\t\tBad Review\n",
      "False\t\t\tBad Review\n",
      "False\t\t\tBad Review\n",
      "True\t\t\tGood Review\n",
      "True\t\t\tGood Review\n",
      "True\t\t\tGood Review\n",
      "False\t\t\tBad Review\n",
      "True\t\t\tGood Review\n",
      "True\t\t\tGood Review\n",
      "True\t\t\tGood Review\n"
     ]
    }
   ],
   "source": [
    "# make predictions with the model\n",
    "\n",
    "probability_predictions = model.predict(X_test_tfidf.toarray())\n",
    "\n",
    "print(\"Predictions for the first 15 examples:\")\n",
    "print(\"Probability\\t\\t\\tClass\")\n",
    "for i in range(0,15):\n",
    "    if probability_predictions[i] >= .5:\n",
    "        class_pred = \"Good Review\"\n",
    "    else:\n",
    "        class_pred = \"Bad Review\"\n",
    "    print(str(probability_predictions[i]) + \"\\t\\t\\t\" + str(class_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review #1:\n",
      "\n",
      "This classic chronicle of Presidential campaigns, from the get-go to contemporary times, has the unusual virtue of being useful either as a collection of short readable chapters - each just the right size for a daily bus or train ride - or as a reference source. Reading this in the wake of Monicagate and the Florida Recount, it's instructive to read the history of Grover Cleveland, who seems to have features of BOTH past Democratic candidates. Like Clinton, he had his scandals - fathering an illegitimate child. Like Gore, his career was rudely interrupted by an election which he won on popular votes but lost, in a hotly contested, knife-edge electoral college tally\n",
      "\n",
      "\n",
      "Prediction: Is this a good review? True\n",
      "\n",
      "Actual: Is this a good review? True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check an actual review\n",
    "\n",
    "print('Review #1:\\n')\n",
    "print(X_test.to_numpy()[100])\n",
    "\n",
    "goodReview = True if probability_predictions[100] >= .5 else False\n",
    "    \n",
    "print('\\nPrediction: Is this a good review? {}\\n'.format(goodReview))\n",
    "\n",
    "print('Actual: Is this a good review? {}\\n'.format(y_test.to_numpy()[100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "After checking different reviews, the model seems to accurately predict the sentiment of the review for the majority of the time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
